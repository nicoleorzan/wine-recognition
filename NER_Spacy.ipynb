{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entities Recognition with Spacy for Wines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the task of annotation?\n",
    "\n",
    "(from https://spacy.io/universe/project/video-spacys-ner-model, Matthew Honnibal, creator of Spacy)\n",
    "\n",
    "Words defined in term of words defined in other words...  Or, better:\n",
    "\n",
    "*\"Trying to learn Chinese by reading in Chinese to Chinese dictionary\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's so hard about Named Entity Recognition??\n",
    "\n",
    "* Structured prediction $\\longrightarrow$ super interesting!\n",
    "\n",
    "* Knowledge intensive $\\longrightarrow$ pretty cool?\n",
    "\n",
    "* Mix of easy and hard cases $\\longrightarrow$ super frustrating..\n",
    "\n",
    "The problem is that even if you have a super powerful algorithm, it will maybe perform quite the same as an easy one.\n",
    "\n",
    "---\n",
    "\n",
    "### How does spacy solve named entities recognition tasks? (Summary)\n",
    "\n",
    "**Word embedding**: Each word is embedded in a representation of this type: ( norm | prefix | suffix | shape ).<br> The sentence/text embedded like this for each word is then passed to an hash function which outputs a 128 units vector for each word. This embedding for the single word is then passed to a multi-layered perceptron with one hidden layer and the Maxout unit (layer where the activation function is the max of the inputs), which outputs a vector of the same length.\n",
    "\n",
    "<img src=\"Images/one.png\" alt=\"drawing\" style=\"width:450px;\"/>\n",
    "\n",
    "Then, to embed the neighboring words, Spacy uses **Convolutional Neural Networks**.<br>\n",
    "Basically, it recalculates the words vector based on the context:<br>\n",
    "For each word creates a **trigram** (with neighbors) and then feeds it into the CNN, passes it to a multi-layered perceptron and comes back to the original words size. With this process we relearn what this word means on its neighbors.\n",
    "\n",
    "In the next layer you take informations from two words on each side, and so on. There is a decaying impact on the informaton if it comes from neighbors which are more far away, because the information it is filtered by the nearer neighbors.\n",
    "\n",
    "\n",
    "<img src=\"Images/two.png\" alt=\"drawing\" style=\"width:450px;\"/>\n",
    "\n",
    "\n",
    "Then the vectors for each word (which compose a matrix) are passed to an **attenton layer**:\n",
    "it is a kind of neural network layer (used a lot in translation tasks), which selects only the piece of the phrase needed to work on the current word.\n",
    "which sumamrizes this matrix to a single vector.\n",
    "\n",
    "<img src=\"Images/four.png\" alt=\"drawing\" style=\"width:450px;\"/>\n",
    "\n",
    "As a \"prediction\" layer we have a simple multi-layered perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import decaying\n",
    "from spacy.lang.it.examples import sentences\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# python3 -m spacy download en\n",
    "spacy.load(\"en\")\n",
    "# python3 -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_md') #lg blocca tutto, 1 Gb di modello\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#from fuzzywuzzy import process\n",
    "#from fuzzywuzzy import fuzz\n",
    "#from fuzzysearch import find_near_matches\n",
    "\n",
    "import json\n",
    "import copy\n",
    "from itertools import islice\n",
    "\n",
    "import nltk.stem as stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(\"saved_things/\" + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(\"saved_things/\" + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GRAPES\n",
    "with open('knowledge/grapes.json') as json_data:\n",
    "    grapes = json.load(json_data)\n",
    "    json_data.close()\n",
    "    grapes = grapes['grapes']\n",
    "\n",
    "grapes = [i.lower() for i in grapes]\n",
    "gr = [\" \" + grape + \" \" for grape in grapes]\n",
    "\n",
    "\n",
    "# WINE KIND\n",
    "with open('knowledge/kind.json') as json_data:\n",
    "    kind = json.load(json_data)\n",
    "    json_data.close()\n",
    "    kind = kind['kind']\n",
    "\n",
    "kind = [i.lower() for i in kind]\n",
    "kind = [\" \" + i.lower()+\" \" for i in kind]\n",
    "\n",
    "\n",
    "# ITA WINENAME\n",
    "with open('knowledge/appellations-it.json') as json_data:\n",
    "    winename = json.load(json_data)\n",
    "    json_data.close()\n",
    "    winename = winename['appellations']\n",
    "    \n",
    "winename = [i.lower() for i in winename]\n",
    "\n",
    "\n",
    "# USA WINENAME\n",
    "winename_usa_tmp = pd.read_csv('knowledge/appellations-usa.csv')\n",
    "winename_usa_tmp.iloc[:,0] = winename_usa_tmp.iloc[:,0] .map(lambda x: x.replace(' AVA',''))\n",
    "\n",
    "winename_usa = list(winename_usa_tmp.iloc[:,0])\n",
    "winename_usa = [x.lower() for x in winename_usa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1435"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### LOADING LIST OF JSONS WITH OCR FROM PYTHON\n",
    "\n",
    "directory = \"saved_things/\"\n",
    "\n",
    "ocrs_dict = {}\n",
    "with open(directory + 'ocrs_dictionary_improved.json' ) as json_data:\n",
    "    ocrs_dict = json.load(json_data)\n",
    "    json_data.close()\n",
    "\n",
    "# taking a fraction of ocrs to create first part of train set\n",
    "ocrs_dict_slice = {}\n",
    "INDICE = int(len(ocrs_dict)/6*5)\n",
    "for i in take(INDICE, ocrs_dict.items()):\n",
    "    ocrs_dict_slice[i[0]] = i[1]\n",
    "\n",
    "# taking remaining ocrs as test set\n",
    "test = {}\n",
    "for i in take(len(ocrs_dict), ocrs_dict.items()):\n",
    "    if int(i[0]) > INDICE:\n",
    "        test[i[0]] = i[1]\n",
    "        \n",
    "        \n",
    "# ================================================================        \n",
    "# ================================================================\n",
    "\n",
    "\n",
    "#### LOADING REVIEWS\n",
    "\n",
    "with open(\"knowledge/\" + 'winemag-data-130k-v2.json') as json_data:\n",
    "    reviews = json.load(json_data)\n",
    "    json_data.close()\n",
    "    \n",
    "# indexing reviews correctly to append them to ocrs dictionary\n",
    "reviews_dict = {}\n",
    "tmp = INDICE + 2\n",
    "for i in reviews:\n",
    "    reviews_dict[tmp] = i['description']\n",
    "    tmp += 1\n",
    "#print(len(reviews_dict))\n",
    "\n",
    "# taking first 1000 reviews\n",
    "tmp = {}\n",
    "for i in take(1000, reviews_dict.items()): \n",
    "    tmp['{}'.format(i[0])] = i[1]\n",
    "\n",
    "reviews_dict = tmp\n",
    "\n",
    "\n",
    "\n",
    "# PUTTING WINE OCR AND SOME REVIEWS TOGHETER TO CREATE A BIGGER TRAIN LIST \n",
    "train = {**ocrs_dict_slice, **reviews_dict}\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING LIST OF 600 JSONS WITH OCR FROM GOOGLE\n",
    "directory = \"/home/nicole/Data Science/Software dev/esteco_project/ocr_google/\"\n",
    "\n",
    "jsons_google = {}\n",
    "for filename in os.listdir(directory):\n",
    "    tmp = re.findall('\\d+',\"{}\".format(filename))[0]\n",
    "    with open(directory + \"{}\".format(filename) ) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        json_data.close()\n",
    "        jsons_google[tmp] = d[0]['description'].lower()\n",
    "        \n",
    "jsons = jsons_google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists of Label for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORG = [\"Winery\", \"Company\", \"Companies\", \"Cellar\"]\n",
    "\n",
    "loc = [\"Mountain\", \"River\", \"Lake\", \"Lakes\", \"Earth\", \"Trees\", \"Castle\", \"Terrain\", \"Garden\"]\n",
    "\n",
    "places = [\"Missouri\", \"City\", 'san benito county', \"new york\", \"COLCHAGUA VALLEY\", \"san luis obispo\",\\\n",
    "          \"Italy\", \"New Zealand\", \"St. Louis County\", \"Columbia Valley\", \"Carolina\", \"Spain\", \"Italia\", \\\n",
    "          \"France\", \"Slovenija\", \"Australia\", \"Chile\", \"España\", \"USA\", \"Canada\", \"Oregon\", \"California\",\\\n",
    "          \"Europe\", \"NAPA\", \"Portland\", \"yorkville\", \"Santa Clara County\", \"monterey county\", \"Oakville\",\\\n",
    "          \"Walla walla\", \"St Helena\", \"santa barbara county\", \"Saint Helena\", \"New Mexico\", \"Manhasset\"]\n",
    "\n",
    "fruit = [\"Honey\", \"Fruit\", \"Fruits\", \"Strawberry\", \"Raspberry\", \"Apple\", \"Citrus\", \"Vanilla\", \"Pineapple\", \"Pear\", \"Mint\", \"Plum\", \"Blackberry\", \"Cherry\", \"Melon\", \"Peach\", \"Lemon\", \"Lime\", \"Mango\"]\n",
    "\n",
    "product = [\"Bottle\", \"Beverages\", \" Car \", \"Cheese\", \"Meat\"]\n",
    "\n",
    "year = list(map(str, range(1600, 2019)))\n",
    "year = [\" \"+str(i)+\" \" for i in year]\n",
    "\n",
    "alcohol = np.arange(10, 18, 0.1).round(2)\n",
    "alcohol = [str(i)+\"%\" for i in alcohol]\n",
    "\n",
    "ORG = [x.lower() for x in ORG]\n",
    "loc = [x.lower() for x in loc]\n",
    "places = [x.lower() for x in places]\n",
    "fruit = [x.lower() for x in fruit]\n",
    "product = [x.lower() for x in product]\n",
    "\n",
    "with open(\"knowledge/list_aromas.txt\", \"rb\") as fp:\n",
    "    aromas = pickle.load(fp)\n",
    "\"\"\"aromas = aromas + ['citrus', 'black fruit', 'red fruit', 'spices', 'dried fruit','dried sage', 'floral', 'cork', \\\n",
    "                   'tropical fruit', 'juicy', 'blackberry', 'berry', 'savory herb', 'dark plum', 'herbal',\\\n",
    "                   'candied berry', 'spice', 'fruity', 'fresh', 'spicy','black cherry', 'espresso', 'buttercream',\\\n",
    "                   'mineral','baked plum','acidity','pepper','clove','red fruit','berry','white flower','licorice', \\\n",
    "                   'coffee beans', 'yellow flower', 'yellow fruit', 'yellow-fruit','berry fruit', 'berry fruits',\\\n",
    "                   'tannins', 'red berry fruits'] + fruit\"\"\"\n",
    "    \n",
    "taste = [\"Semi-Dry\", \"SemiDry\", \"Semi Dry\", \"Medium-Dry\", \"MediumDry\",  \"Medium Dry\", \"Dry\", \\\n",
    "         \"Semi-Sweet\", \"SemiSweet\", \"Semi Sweet\", \"Sweet\", \"Medium-Sweet\", \"MediumSweet\", \"Medium Sweet\", \\\n",
    "         \"Full-Bodied\", \"FullBodied\", \"Full Bodied\", \"Medium-Bodied\", \"MediumBodied\", \"Medium Bodied\", \\\n",
    "         \"Crisp\", \"Balance\", 'Sparkling', 'rough', 'tannic', 'tannins', 'smooth']\n",
    "taste = [x.lower() for x in taste]\n",
    "\n",
    "vineyards = ['gimelli vineyards', 'Boenker Hill Vineyard', 'edna valley vineyard', 'laetitia vineyard', \\\n",
    "             'Lockwood Vineyards', 'Skyfall Vineyard', 'sonoma coast vineyards', 'balletto vineyards', \\\n",
    "             'Becker Vineyards', 'mount baker vineyards', 'guerra vineyards', 'camelot vineyard', 'pedregal vineyard', \\\n",
    "             'chalone vineyard', 'adastra vineyard', 'sangiovese vineyards', 'weir vineyard', \\\n",
    "             'spring hill vineyards', 'adelsheim vineyard', 'gunsalus vineyard', 'leaping horse vineyards', \\\n",
    "             'reed vineyard', 'ADIRONDACK WINERY', 'Chapelle Winery']\n",
    "vineyards = [x.lower() for x in vineyards]\n",
    "\n",
    "#food = [seafood, pot, chicken, ssh, creamy sauce dishes,\\ntight salads or sandwiches and song cheeses meat dish]\n",
    "##['toasty', 'creamy', 'loamy', 'minerally', 'minerality', 'toasted', 'bodied', 'buttered toast'] acidity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to create the labeled list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_setup(sent):\n",
    "    \n",
    "    '''preparing, for each sentence, a tuple with \n",
    "    the sentence and the list of entities (empty)'''\n",
    "    \n",
    "    tupla = (sent.lower(), {'entities': [] })\n",
    "    return tupla\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "def tag(label, list_of_words, tupla):\n",
    "    \n",
    "    '''function to tag a single sentence searching for words and labeling them, \n",
    "    assuring to not overlabel entities which are already labeled'''\n",
    "    \n",
    "    sent, entities = tupla\n",
    "    #regex = re.compile('[^a-zA-Z]')\n",
    "    st = stem.RegexpStemmer('ing$|s$|y$|ly$|ed$', min=4)\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        if word in sent:\n",
    "            all_recurrencies_word = re.finditer(word, sent)\n",
    "            \n",
    "            # tagging all recurrency of the word(\\words) in the sentence\n",
    "            for i, word_recurrence in enumerate(all_recurrencies_word):\n",
    "                if all(ent[0][0] != word_recurrence.start() for ent in entities['entities']):\n",
    "                    new_ent = [(word_recurrence.start(), word_recurrence.end(), label, word)]\n",
    "                    entities['entities'].append(new_ent)\n",
    "                                    \n",
    "    return(tupla)\n",
    "# ==================================================================\n",
    "    \n",
    "    \n",
    "# ============ MAIN FUNCTION =============\n",
    "\n",
    "def label_tagging(sentences, dict_of_tags):\n",
    "    \n",
    "    '''main function which loops over the sentences \n",
    "    dictionary to tag the entities into tuples'''\n",
    "\n",
    "    tuples = []\n",
    "    for key, sent in sentences.items():\n",
    "        tupla = sentence_setup(sent)\n",
    "        for key, val in dict_of_tags.items():\n",
    "            tupla = tag(key, val, tupla)\n",
    "        tuples.append(tupla)\n",
    "            \n",
    "    return(tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating labeled list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_tags = {\"VINE\": grapes, \"LOC\": loc, \"WINENAME\": winename + winename_usa, \"GPE\": places, \\\n",
    "                \"ORG\": ORG,  \"DATE\": year, \"KIND\": kind, \"AROMA\": aromas, \"TASTE\": taste, \\\n",
    "                \"VINEYARD\": vineyards}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch labeling and save, or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_tagging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9586bff9eb7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabeled_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_tagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_of_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabeled_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_tagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_of_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_tagging' is not defined"
     ]
    }
   ],
   "source": [
    "labeled_train = label_tagging(train, dict_of_tags)\n",
    "labeled_test = label_tagging(test, dict_of_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_obj(labeled_train, \"labeled_train\")\n",
    "#save_obj(labeled_test, \"labeled_test\")\n",
    "#save_obj(lab, \"labeled_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled_train = load_obj(\"labeled_train\")\n",
    "#labeled_test = load_obj(\"labeled_test\")\n",
    "#labeled_reviews = load_obj(\"labeled_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "# ============================================================================================= \n",
    "\n",
    "\n",
    "\n",
    "def get_batches(train_data, model_type):\n",
    "    max_batch_sizes = {'tagger': 32, 'parser': 16, 'ner': 16, 'textcat': 64}\n",
    "    max_batch_size = max_batch_sizes[model_type]\n",
    "    if len(train_data) < 1000:\n",
    "        max_batch_size /= 2\n",
    "    if len(train_data) < 500:\n",
    "        max_batch_size /= 2\n",
    "    batch_size = compounding(1, max_batch_size, 1.001)\n",
    "    batches = minibatch(train_data, size=batch_size)\n",
    "\n",
    "    return batches\n",
    "# ============================================================================================= \n",
    "\n",
    "\n",
    "\n",
    "def main(LABEL, TRAIN_DATA, drop, model=None, new_model_name='wines', output_dir=None, n_iter=10):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model) #  load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    if (isinstance(LABEL, list)):\n",
    "        for L in LABEL:\n",
    "            ner.add_label(L)\n",
    "    else: \n",
    "        ner.add_label(LABEL)\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        #'begin_training' initializes the models, so it'll zero out existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    losses_list = []\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            # batches = get_batches(TRAIN_DATA, \"ner\")\n",
    "            dropout = decaying(0.7, 0.2, 1e-1)\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(50., 500., 30.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                drop = next(dropout)\n",
    "                #print(drop)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=drop,\n",
    "                           losses=losses)\n",
    "            losses_list.append(losses['ner'])\n",
    "            print('iteration ', itn, ', Losses', losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = test['451']\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    print(\"==================\")\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "        \n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        \"\"\"print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\"\"\"\n",
    "    return losses_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start model training\n",
    "(Do not run it if you're not sure and brave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_md'\n",
      "iteration  0 , Losses {'ner': 167685.22723388672}\n",
      "iteration  1 , Losses {'ner': 183977.1552886963}\n"
     ]
    }
   ],
   "source": [
    "# OLD PC!!!\n",
    "L = [\"VINE\", \"KIND\", \"AROMA\", \"WINENAME\", \"ALCOHOL\", \"TASTE\", \"VINEYARD\"]\n",
    "\n",
    "loss = main(LABEL = L, TRAIN_DATA = labeled_train, drop = 0.35, model = 'en_core_web_md', n_iter = 5, output_dir = \"output_model_ocr_google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_lg'\n",
      "iteration  0 , Losses {'ner': 0.032081051766908786}\n",
      "iteration  1 , Losses {'ner': 0.01871299426056794}\n",
      "iteration  2 , Losses {'ner': 0.02453815328794917}\n",
      "iteration  3 , Losses {'ner': 0.028435918107788893}\n",
      "iteration  4 , Losses {'ner': 0.017151243498801705}\n",
      "iteration  5 , Losses {'ner': 0.0170900036087005}\n",
      "iteration  6 , Losses {'ner': 0.016519607813393122}\n",
      "iteration  7 , Losses {'ner': 0.01316878324733084}\n",
      "iteration  8 , Losses {'ner': 0.01531941621851729}\n",
      "iteration  9 , Losses {'ner': 0.015748691503439716}\n",
      "iteration  10 , Losses {'ner': 0.015846964844058675}\n",
      "iteration  11 , Losses {'ner': 0.012845607702033135}\n",
      "iteration  12 , Losses {'ner': 0.019628980061497714}\n",
      "iteration  13 , Losses {'ner': 0.020614732658941648}\n",
      "iteration  14 , Losses {'ner': 0.02374336332616167}\n",
      "iteration  15 , Losses {'ner': 0.014534122405633454}\n",
      "iteration  16 , Losses {'ner': 0.01778060311784202}\n",
      "iteration  17 , Losses {'ner': 0.016252473854649452}\n",
      "iteration  18 , Losses {'ner': 0.012603895845813895}\n",
      "iteration  19 , Losses {'ner': 0.011619627975875346}\n",
      "iteration  20 , Losses {'ner': 0.010783864274566568}\n",
      "iteration  21 , Losses {'ner': 0.006956841058858743}\n",
      "iteration  22 , Losses {'ner': 0.011106599604318035}\n",
      "iteration  23 , Losses {'ner': 0.019359658435860183}\n",
      "iteration  24 , Losses {'ner': 0.01565950157030649}\n",
      "Entities in 'This shows jelly-like flavors of orange and pear, with some earthy tones. The mouthfeel is soft and there's a bounty of oak in the form of buttered toast and caramel notes.'\n",
      "==================\n",
      "AROMA orange\n",
      "AROMA pear\n",
      "AROMA oak\n",
      "AROMA toast\n",
      "AROMA caramel\n",
      "Saved model to output_model_ocr_google\n"
     ]
    }
   ],
   "source": [
    "#L = [\"VINE\", \"KIND\", \"AROMA\", \"WINENAME\", \"ALCOHOL\", \"TASTE\", \"VINEYARD\"]\n",
    "\n",
    "#loss = main(LABEL = L, TRAIN_DATA = labeled_train, drop = 0.35, model = 'en_core_web_md', n_iter = 25, output_dir = \"output_model_ocr_google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(0, len(loss)))\n",
    "plt.plot(x, loss, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Testing saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"output_model_ocr_google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uas': 0.0, 'las': 0.0, 'ents_p': 0.0, 'ents_r': 0.0, 'ents_f': 0.0, 'tags_acc': 0.0, 'token_acc': 100.0}\n"
     ]
    }
   ],
   "source": [
    "def evaluate(nlp, examples):\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        text_entities = []\n",
    "        for entity in annot.get('entities'):\n",
    "            #if ent in entity:\n",
    "            text_entities.append(entity)\n",
    "        doc_gold_text = nlp.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=text_entities)\n",
    "        pred_value = nlp(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores\n",
    "\n",
    "\n",
    "results = evaluate(nlp, labeled_test[0:4])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= STE\n",
      "CHAPELLE\n",
      "CHARDONNAY\n",
      "\n",
      "Overlooking the farms and orchards of the Snake\n",
      "River Valley are Ste. Chapelle Winery’s premium\n",
      "vineyards. Since 1976 we’ve recognized that cool\n",
      "winters, hot summers, volcanic ash soils and the\n",
      "vineyards’ lofty elevations lend to incredible flavor\n",
      "and balance. These distinct characteristics can be\n",
      "found in our refreshing Chardonnay. Enjoy beautiful\n",
      "pineapple, pear and toasty oak notes with grilled\n",
      "seafood, roast chicken or mild cheeses.\n",
      "\n",
      " \n",
      "\n",
      "PRODUCED & BOTTLED BY\n",
      "STE. CHAPELLE, CALDWELL, IDAHO\n",
      "\n",
      "GOVERNMENT WARNING: (1) ACCORDING TO\n",
      "THE SURGEON GENERAL, WOMEN SHOULD\n",
      "NOT DRINK ALCOHOLIC BEVERAGES DURING\n",
      "PREGNANCY BECAUSE OF THE RISK OF BIRTH\n",
      "DEFECTS. (2) CONSUMPTION OF ALCOHOLIC\n",
      "BEVERAGES IMPAIRS YOUR ABILITY TO DRIVE\n",
      "A CAR OR OPERATE MACHINERY, AND MAY\n",
      "CAUSE HEALTH PROBLEMS.\n",
      "\n",
      "750mL CONTAINS SULFITES ° \n",
      "\n",
      "\n",
      "CHARDONNAY VINE\n",
      "River LOC\n",
      "Winery ORG\n",
      "balance TASTE\n",
      "Chardonnay VINE\n",
      "pineapple AROMA\n",
      "pear AROMA\n",
      "oak AROMA\n"
     ]
    }
   ],
   "source": [
    "file = test['452']\n",
    "print(file, \"\\n\\n\")\n",
    "doc = nlp(file)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp):\n",
    "    return tp / (tp + fp)\n",
    "    \n",
    "def recall(tp, fn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def fbeta_score(beta, tp, fn, fp): \n",
    "    return  ( (1+beta**2) * tp ) / ( (1 + beta**2) * tp + beta**2 * fn + fp ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 SCORE: 0.9230769230769231\n",
      "0.9230769230769231\n",
      "[0.9230769230769231]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_list = []; fp_list = []; f_score = []; fn_list = []; weights = []\n",
    "\n",
    "\n",
    "for num, entry in enumerate(labeled_test[0:1]):\n",
    "    file = labeled_test[num]\n",
    "\n",
    "    exact = []\n",
    "    for i in file[1]['entities']:\n",
    "        exact.append([i[2], file[0][i[0]:i[1]].lower()])\n",
    "\n",
    "    doc2 = nlp(file[0])\n",
    "    outcome = []\n",
    "    for ent in doc2.ents:\n",
    "        outcome.append([ent.label_, ent.text.lower()])\n",
    "    #print(\"exact:\", exact, \"\\n\"); print(\"outcome: \" ,outcome)\n",
    "\n",
    "    tp = 0\n",
    "    L = len(exact)\n",
    "    if L != 0:\n",
    "        for i in exact:\n",
    "            for j in outcome:\n",
    "                if i[0] == j[0] and i[1] in j[1]: # in or == ??\n",
    "                    tp += 1\n",
    "                    outcome.remove(j)\n",
    "        fn = len(exact) - tp\n",
    "        fp = len(outcome)\n",
    "        fp_list.append(fp)\n",
    "        tp_list.append(tp)\n",
    "        fn_list.append(fn)\n",
    "        f_score.append(fbeta_score(1, tp, fn, fp))\n",
    "        weights.append(L)\n",
    "\n",
    "        \n",
    "\n",
    "tp = np.mean(tp_list)\n",
    "fn = np.mean(fn_list)\n",
    "fp = np.mean(fp_list)\n",
    "beta = 1\n",
    "print(\"F-{}\".format(beta), \"SCORE:\", fbeta_score(beta, tp, fn, fp))\n",
    "print(np.mean(f_score))\n",
    "print(f_score)\n",
    "len(f_score)\n",
    "len(weights)\n",
    "np.average(f_score, weights = weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old things "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNUSED \n",
    "\n",
    "def word_in_list(lista, sentence, label, dictionar):\n",
    "    #st = RegexpStemmer('ing$|s$|y$|ly$|ed$', min=4)\n",
    "    found_words = []\n",
    "    for word in lista:\n",
    "        word = word.lower()\n",
    "        if (label == 'AROMA'):\n",
    "            \"\"\"if (st.stem(word) in sentence.lower()):\n",
    "                print(\"word: \", word, \"stem:\", st.stem(word))\n",
    "            #if (word in sentence.lower()):\n",
    "                dictionar['entities'].append( (sentence.lower().index(st.stem(word)), sentence.lower().index(st.stem(word)) + len(word), label ))\n",
    "                if word not in found_words:\n",
    "                    found_words.append(word)\"\"\"\n",
    "        #else:\n",
    "        if (word in sentence.lower()):\n",
    "            dictionar['entities'].append( (sentence.lower().index(word), sentence.lower().index(word) + len(word), label, word ))\n",
    "            if word not in found_words:\n",
    "                found_words.append(word)\n",
    "            \n",
    "    return (found_words, dictionar)\n",
    "#### ============================================================================================= \n",
    "\n",
    "\n",
    "\n",
    "def entity_search(list_name, lista, entities, sentence, label, dictionar):\n",
    "    \n",
    "    found_words = []\n",
    "    for ent in entities:\n",
    "        for word in lista:\n",
    "            if (ent.text.lower() == word):\n",
    "                dictionar['entities'].append(  ( ent.start_char, ent.end_char, label))\n",
    "                if word not in found_words:\n",
    "                    found_words.append(word)\n",
    "\n",
    "    return (found_words, dictionar)\n",
    "#### ============================================================================================= \n",
    "\n",
    "\n",
    "\n",
    "def percent_check(entities, sentence, dictionar):\n",
    "    for ent in entities:\n",
    "        if (ent.label_ == \"PERCENT\"):\n",
    "            for word in ent.text.split():\n",
    "                if (\"%\" in word and re.search(r'\\d', word)):\n",
    "                    if (float(re.sub('[^0-9.]', '', word)) > 6 and float(re.sub('[^0-9.]', '', word)) < 15):\n",
    "                        #num = re.sub('[^0-9.]', '', word)\n",
    "                        #if ( float(num) > 6 and  float(num) < 16):\n",
    "                         dictionar['entities'].append(  ( ent.start_char, ent.end_char, \"ALCOHOL\"))# (  (sent.string.strip(), {\"entities\": [( ent.start_char, ent.end_char, \"ALCOHOL\")]})  )\n",
    "    return dictionar\n",
    "#### ============================================================================================= \n",
    "\n",
    "\n",
    "\n",
    "def entity_check(doc):\n",
    "    print(\"TEXT:\")\n",
    "    print(doc)\n",
    "    print(\"ENTITIES:\")\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "    print(\"NOUN CHUNKS\")\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(\"text: \", chunk.text, \", root:\", chunk.root.text, \", root dependency:\", chunk.root.dep_, \\\n",
    "              \", root head text: \", chunk.root.head.text)\n",
    "#### ============================================================================================= \n",
    "\n",
    "\n",
    "\n",
    "def entity_site(entities, sentence, dictionar):\n",
    "    for ent in entities:\n",
    "        if (\"www\" in ent.text.lower() or \"www\" in ent.text.lower().replace(\" \", \"\") or \".com\" in ent.text.lower()):\n",
    "            dictionar['entities'].append(  ( ent.start_char, ent.end_char, \"SITE\"))\n",
    "\n",
    "def fuzzy_extract(qs, ls, threshold):\n",
    "    '''fuzzy matches 'qs' in 'ls' and returns list of \n",
    "    tuples of (word,index)\n",
    "    '''\n",
    "    for word, _ in process.extractBests(qs, (ls,), score_cutoff=threshold):\n",
    "        #print('word {}'.format(word))\n",
    "        for match in find_near_matches(qs, word, max_l_dist=1):\n",
    "            match = word[match.start:match.end]\n",
    "            #print('match {}'.format(match))\n",
    "            index_start = ls.find(match)\n",
    "            index_end = ls.find(match)+len(match)\n",
    "            yield (match, index_start, index_end)\n",
    "#### =============================================================================================             \n",
    "    \n",
    "    \n",
    "    \n",
    "def fuzzy_extract_on_list(lista, sentence, threshold, dictionar, label):\n",
    "    \n",
    "    found_words = []\n",
    "    for query in lista:\n",
    "        if (len(query) > 4):\n",
    "            for match, index_start, index_end in fuzzy_extract(\" \" + query.lower() + \" \", sentence.lower(), threshold):\n",
    "                dictionar['entities'].append(  ( index_start, index_end, label))\n",
    "                #print(\"match with: \", query, \"and \", match)\n",
    "                if match not in found_words:\n",
    "                    found_words.append(match)\n",
    "                #print('for {}, match: {}\\nindex_start: {}\\nindex_end: {}, sentence: {}\\n'.format(query.lower(), match, index_start, index_end, sentence))\n",
    "    return (found_words, dictionar)\n",
    "\n",
    "_=\"\"\"large_string = jsons['181'].lower()\n",
    "query_string = \"Pinot Noir\".lower()\n",
    "queries = [\"Pinot Noir\", \"Mountain\"]\n",
    "\n",
    "for query in queries:\n",
    "    print(\"=====>\",query)\n",
    "    for match, index_start, index_end in fuzzy_extract(query.lower(), large_string, 50):\n",
    "        print('for {}, match: {}\\nindex_start: {}\\nindex_end: {}'.format(query, match, index_start, index_end))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNUSED\n",
    "def labeling_function(jsons):\n",
    "\n",
    "    prova_train = []\n",
    "\n",
    "    num = 1\n",
    "    #entities_found_total = 0\n",
    "    for key, value in jsons.items():\n",
    "        if (int(key) < 130571): # TOT 130571\n",
    "            if (int(key)%50 == 0): print(num, \"============ key: \", key , \"============\")\n",
    "            num += 1\n",
    "            wine_found = False\n",
    "            entity_added = 0\n",
    "\n",
    "            dic = {\"entities\": []}\n",
    "\n",
    "            sentence = value\n",
    "            tmp = nlp(sentence)\n",
    "\n",
    "\n",
    "            for list_name in list_of_entites:\n",
    "                #print(\"======== \", list_name, \"========\")\n",
    "                list_tmp = copy.deepcopy(list_of_entites[list_name])\n",
    "\n",
    "                # ENTRO NELLE ENTITA DELLA FRASE GIA TROVATE E CONTROLLO SE POSSO RECUPERARE QUALCOSA\n",
    "                \"\"\"found_words, dic = entity_search(list_name, list_tmp, tmp.ents, sentence, list_name, dic)\n",
    "                #print(\"entity found words:\", found_words)\n",
    "                if (list_name ==\"VINE\" and found_words != []): wine_found = True\n",
    "                if (list_name == \"TASTE\"):\n",
    "                    for word in found_words:\n",
    "                        for full_word in list_tmp:\n",
    "                            if (word in full_word): list_tmp.remove(full_word)\n",
    "                else:\n",
    "                    [list_tmp.remove(i) for i in found_words]\"\"\"\n",
    "\n",
    "                # SE NELLE ENTITA NON TROVO NIENTE, CERCO NELLA FRASE QUALCHE PAROLA IN MODO LINEARE\n",
    "                found_words, dic = word_in_list(list_tmp, sentence, list_name, dic)\n",
    "                if (list_name ==\"VINE\" and found_words != []): wine_found = True\n",
    "                #print(\"linearly found words:\", found_words)\n",
    "                if (list_name == \"TASTE\"):\n",
    "                    for word in found_words:\n",
    "                        for full_word in list_tmp:\n",
    "                            if (word in full_word): list_tmp.remove(full_word)\n",
    "                else:\n",
    "                    [list_tmp.remove(i) for i in found_words]\n",
    "\n",
    "\n",
    "                #if (list_name == \"VINE\" and wine_found == False):\n",
    "                #    found_words, dic = fuzzy_extract_on_list(list_tmp, sentence, 75, dic, list_name)\n",
    "                    #[list_tmp.remove(i) for i in found_words]\n",
    "\n",
    "            # SITE AND ALOCHOL LEVEL A PART \n",
    "            entity_site(tmp.ents, sentence, dic) \n",
    "            word_in_list(alcohol, sentence, \"ALCOHOL\", dic)\n",
    "            del list_tmp\n",
    "\n",
    "            # append results\n",
    "            prova_train.append( (sentence, dic))\n",
    "\n",
    "    return prova_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
